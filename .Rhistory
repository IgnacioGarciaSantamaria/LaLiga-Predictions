# saco los puntos de cada equipo en cada jornada
df_raw[i,"HP" ] <- ifelse(df_raw[i,"FTR" ]=="H",3,ifelse(df_raw[i,"FTR" ]=="A",0,1))
df_raw[i,"AP" ] <- ifelse(df_raw[i,"FTR" ]=="A",3, ifelse(df_raw[i,"FTR" ]=="H",0,1))
date <- df_raw[(df_raw$Date < df_raw[i,"Date"]),]
home <- date[(date$HomeTeam==df_raw[i,"HomeTeam"]),]
away <- date[(date$AwayTeam==df_raw[i,"AwayTeam"]),]
# solo quiero los tres ultimos partidos
if (nrow(home)>3){
home <- tail(home, 3)
}
if (nrow(away)>3){
away <- tail(away, 3)
}
# para los primeros partidos ( no hay suficientes para la media)
if (dim(home)[1] == 0) {
home <- df_raw[i,]
}
if (dim(away)[1] == 0) {
away <- df_raw[i,]
}
df_raw[i,"FTHG_avg" ] <- mean(home[,"FTHG"])
df_raw[i,"FTAG_avg" ] <- mean(away[,"FTAG"])
df_raw[i,"HTHG_avg" ] <- mean(home[,"HTHG"])
df_raw[i,"HTAG_avg" ] <- mean(away[,"HTAG"])
df_raw[i,"HS_avg" ] <- mean(home[,"HS"])
df_raw[i,"AS_avg" ] <- mean(away[,"AS"])
df_raw[i,"HST_avg" ] <- mean(home[,"HST"])
df_raw[i,"AST_avg" ] <- mean(away[,"AST"])
df_raw[i,"HF_avg" ] <- mean(home[,"HF"])
df_raw[i,"AF_avg" ] <- mean(away[,"AF"])
df_raw[i,"HC_avg" ] <- mean(home[,"HC"])
df_raw[i,"AC_avg" ] <- mean(away[,"AC"])
df_raw[i,"HY_avg" ] <- mean(home[,"HY"])
df_raw[i,"AY_avg" ] <- mean(away[,"AY"])
df_raw[i,"HR_avg" ] <- mean(home[,"HR"])
df_raw[i,"AR_avg" ] <- mean(away[,"AR"])
df_raw[i,"HP_avg" ] <- mean(home[,"HP"])
df_raw[i,"AP_avg" ] <- mean(away[,"AP"])
}
return(df_raw)
}
liga <- get_last_3()
liga_avg <- liga
liga <- liga_avg[,c("Season","Date","HomeTeam","AwayTeam","FTHG_avg","FTAG_avg","HS_avg","AS_avg","HST_avg","AST_avg",
"HC_avg","AC_avg","HF_avg","AF_avg","HY_avg","AY_avg","HR_avg","AR_avg","HP_avg","AP_avg","B365A","B365H","B365D")]
liga$HomeTeam <- as.factor(liga$HomeTeam)
liga$AwayTeam <- as.factor(liga$AwayTeam)
liga$Season <- as.factor(liga$Season)
index <- liga$Season %in% "2018-2019"
test <- liga[index,]
train <- liga[-index,]
library(ggplot2) # Required for plotting with ggplot
library(e1071) #Required for SVM
library(vcd) # For mosaic
library(datasets) # Required for loading iris data
set.seed(1)
svr_train<-tune("svm",FTAG_avg ~ HomeTeam+AwayTeam+AS_avg+AST_avg+
AC_avg+AP_avg+B365A+B365H+B365D,
data=train[1:2000,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
svr_train<-tune("svm",FTAG_avg ~ HomeTeam+AwayTeam+AS_avg+AST_avg+
AC_avg+AP_avg+B365A+B365H+B365D,
data=train[1:200,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
ggplot(data=svr_train$performances, aes(x=cost, y=error,
color=factor(epsilon)))+
geom_line()+
geom_point()+
labs(title="Classification error vs  C & epsilon")+
theme_bw()+theme(legend.position ="bottom")
summary(svr_train)
#Saving the best model found for C, epsilon & gamma
best_model <- svr_train$best.model
summary(best_model)
## Calculate parameters of the SVR Model
#Find value of W
W=t(best_model$coefs) %*% best_model$SV
print(W)
predict.train.svr <- predict(best_model,train)
MSE.svr.train <- mean((train$FTAG_avg - predict.train.svr)^2)
predict.test.svr <- predict(best_model,test)
MSE.svr.test <- mean((test$FTAG_avg - predict.test.svr)^2)
View(train)
svr_train<-tune("svm",FTAG_avg ~ HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D,data=train,
data=train,
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
plot(test$FTAG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(predict.test.svr, type = "p", col = "blue")
error_test<-test$FTAG_avg-predict.test.svr
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=75, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0,5, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0,5, col=c('blue'))
error_test<-test$FTAG_avg-predict.test.svr
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=1, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0,75, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.75, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.25, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.05, col=c('blue'))
.
# Compare predictions vs real values
plot(test$FTAG_avg,predict.test.svr,col='red',
main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
set.seed(1)
svr_train<-tune("svm",FThG_avg ~ FTAG_avg ~ HomeTeam+AwayTeam+HTR+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D,data=train,
data=train[1:200,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
svr_train<-tune("svm",FTHG_avg ~ HomeTeam+AwayTeam+HTR+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D,data=train,
data=train[1:200,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
svr_train<-tune("svm",FTHG_avg ~ HomeTeam+AwayTeam+HS_avg+
HC_avg+HP_avg+B365H+B365D,data=train,
data=train[1:200,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
svr_train<-tune("svm",FTHG_avg ~ HomeTeam+AwayTeam+HS_avg+
HC_avg+HP_avg+B365H+B365D,
data=train[1:200,],
kernel="radial",
ranges=list(
epsilon=seq(0,1,0.1),
cost=c(0.01, 0.1, 1, 5, 10, 20),
gamma=c(0.1, 0.5, 1, 2, 5, 10)))
ggplot(data=svr_train$performances, aes(x=cost, y=error,
color=factor(epsilon)))+
geom_line()+
geom_point()+
labs(title="Classification error vs  C & epsilon")+
theme_bw()+theme(legend.position ="bottom")
summary(svr_train)
#Saving the best model found for C, epsilon & gamma
best_model <- svr_train$best.model
summary(best_model)
## Calculate parameters of the SVR Model
#Find value of W
W=t(best_model$coefs) %*% best_model$SV
print(W)
predict.train.svr <- predict(best_model,train)
MSE.svr.train <- mean((train$FTAG_avg - predict.train.svr)^2)
predict.test.svr <- predict(best_model,test)
MSE.svr.test <- mean((test$FTHG_avg - predict.test.svr)^2)
predict.train.svr <- predict(best_model,train)
MSE.svr.train <- mean((train$FTHG_avg - predict.train.svr)^2)
predict.test.svr <- predict(best_model,test)
MSE.svr.test <- mean((test$FTHG_avg - predict.test.svr)^2)
plot(test$FTHG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(predict.test.svr, type = "p", col = "blue")
error_test<-test$FTHG_avg-predict.test.svr
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.05, col=c('blue'))
# Compare predictions vs real values
plot(test$FTHG_avg,predict.test.svr,col='red',
main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
tree_result<- rpart(formula=FTAG_avg ~ HomeTeam+AwayTeam+HTR+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data=train,
method='anova')
#################################################ARBOL DE REGRESION AWAY
library(MASS)        # for obtaining data
library(tidyverse)  # for data processing
library(rpart)      # for CART decision tree
library(rpart.plot) # for plotting CART
library(caret)      # for confusion matrix and more
library(rsample)    # for data splitting
tree_result<- rpart(formula=FTAG_avg ~ HomeTeam+AwayTeam+HTR+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data=train,
method='anova')
tree_result<- rpart(formula=FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data=train,
method='anova')
# Analysis of cp values in a table
printcp(tree_result, digits=6)
# Error evolution with increasing number of nodes
plotcp(tree_result, lty=2 , col="red", upper="size" )
plotcp(tree_result, lty=2 , col="red", upper="splits" )
best_cp<- tree_result$cptable[which.min(tree_result$cptable[,"xerror"]),"CP"]
tree_pruned<- prune(tree_result, cp=best_cp)
rpart.plot(tree_pruned, type=1, branch=0,tweak=1.3,
fallen.leaves = TRUE,
varlen = 0, faclen = 0)
pred_train <- predict(tree_pruned, newdata = train)
pred_test <- predict(tree_pruned, newdata = test)
MSE.rt.train <- mean((train$FTAG_avg - pred_train)^2)
MSE.rt.test <- mean((test$FTAG_avg - pred_test)^2)
plot(test$FTAG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(pred_test, type = "p", col = "blue")
error_test<-test$FTAG_avg-pred_test
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.05, col=c('blue'))
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.25, col=c('blue'))
# Compare predictions vs real values
plot(test$FTAG_avg,pred_test,col='red',
main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
##############################HOME
tree_result<- rpart(formula=FTHG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data=train,
method='anova')
# Analysis of cp values in a table
printcp(tree_result, digits=6)
# Error evolution with increasing number of nodes
plotcp(tree_result, lty=2 , col="red", upper="size" )
plotcp(tree_result, lty=2 , col="red", upper="splits" )
# Analysis of cp values in a table
printcp(tree_result, digits=6)
best_cp<- tree_result$cptable[which.min(tree_result$cptable[,"xerror"]),"CP"]
tree_pruned<- prune(tree_result, cp=best_cp)
rpart.plot(tree_pruned, type=1, branch=0,tweak=1.3,
fallen.leaves = TRUE,
varlen = 0, faclen = 0)
pred_train <- predict(tree_pruned, newdata = train)
pred_test <- predict(tree_pruned, newdata = test)
MSE.rt.train <- mean((train$FTHG_avg - pred_train)^2)
MSE.rt.test <- mean((test$FTHG_avg - pred_test)^2)
plot(test$FTHG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(pred_test, type = "p", col = "blue")
error_test<-test$FTHG_avg-pred_test
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.25, col=c('blue'))
# Compare predictions vs real values
plot(test$FTHG_avg,pred_test,col='red',
main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
#####################BOOSTING REGRESION AWAY
library(MASS)       # for obtaining data
library(tidyverse)  # for data processing
library(gbm)        # for Boosting algorithms
cv_error  <- vector("numeric")
n_trees <- vector("numeric")
shrinkage <- vector("numeric")
for (i in c(2, 3, 5, 10)) {
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train_TODO,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = i,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
n.minobsinnode <- c(n.minobsinnode,
rep(i, length(tree_boosting$cv.error)))
}
for (i in c(0.001, 0.01, 0.1)) {
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 20000,
interaction.depth = 1,
shrinkage = i,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
shrinkage <- c(shrinkage, rep(i, length(tree_boosting$cv.error)))
}
# Shrinkage optimization
for (i in c(0.001, 0.01, 0.1)) {
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 20000,
interaction.depth = 1,
shrinkage = i,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
shrinkage <- c(shrinkage, rep(i, length(tree_boosting$cv.error)))
}
error <- data.frame(cv_error, n_trees, shrinkage)
ggplot(data = error, aes(x = n_trees, y = cv_error,
color = as.factor(shrinkage))) +
geom_smooth() +
labs(title = "Evolution of the cv-error", color = "shrinkage") +
theme_bw() +
theme(legend.position = "bottom")
tree_boosting <- gbm(FTAG_avg ~ HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = i,
shrinkage = 0.01,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
for (i in c(3, 4, 5, 8)) {
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = i,
shrinkage = 0.01,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
interaction.depth <- c(interaction.depth,
rep(i, length(tree_boosting$cv.error)))
}
# Selecting tree complexity
cv_error  <- vector("numeric")
n_trees <- vector("numeric")
interaction.depth <- vector("numeric")
for (i in c(3, 4, 5, 8)) {
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = i,
shrinkage = 0.01,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
interaction.depth <- c(interaction.depth,
rep(i, length(tree_boosting$cv.error)))
}
error <- data.frame(cv_error, n_trees, interaction.depth)
ggplot(data = error, aes(x = n_trees, y = cv_error,
color = as.factor(interaction.depth))) +
geom_smooth() +
labs(title = "Evolution of the cv-error", color = "interaction.depth") +
theme_bw() +
theme(legend.position = "bottom")
for (i in c(2, 3, 5, 10)) {
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = i,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
n.minobsinnode <- c(n.minobsinnode,
rep(i, length(tree_boosting$cv.error)))
}
#Selecting the minimum number of observations
cv_error  <- vector("numeric")
n_trees <- vector("numeric")
interaction.depth <- vector("numeric")
for (i in c(2, 3, 5, 10)) {
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = i,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, tree_boosting$cv.error)
n_trees <- c(n_trees, seq_along(tree_boosting$cv.error))
n.minobsinnode <- c(n.minobsinnode,
rep(i, length(tree_boosting$cv.error)))
}
# Determination of the number of trees to use
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 10,
shrinkage = 0.01,
n.minobsinnode = 2,
bag.fraction = 0.5,
cv.folds = 5)
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 500,
interaction.depth = 10,
shrinkage = 0.01,
n.minobsinnode = 2,
bag.fraction = 0.5,
cv.folds = 5)
error <- data.frame(cv_error = tree_boosting$cv.error,
n_trees = seq_along(tree_boosting$cv.error))
ggplot(data = error, aes(x = n_trees, y = cv_error)) +
geom_line(color = "blue") +
geom_point(data = error[which.min(error$cv_error),], color = "red") +
labs(title = "Evolution of the cv-error") +
theme_bw()
error[which.min(error$cv_error),]
set.seed(123)
tree_boosting <- gbm(FTAG_avg ~ HomeTeam+AwayTeam+HS_avg+AS_avg+HST_avg+AST_avg+
HC_avg+AC_avg+HF_avg+AF_avg+HY_avg+AY_avg+HR_avg+AR_avg+HP_avg+AP_avg+B365A+B365H+B365D, data = train,
distribution = "gaussian",
n.trees = 485,
interaction.depth = 4,
shrinkage = 0.01,
n.minobsinnode = 3,
bag.fraction = 0.5)
summary(tree_boosting)
importance_pred <- summary(tree_boosting, plotit = FALSE)
ggplot(data = importance_pred, aes(x = reorder(var, rel.inf),
y = rel.inf,
fill = rel.inf)) +
labs(x = "variable", title = "MSE reduction") +
geom_col() +
coord_flip() +
theme_bw() +
theme(legend.position = "bottom")
par(mfrow = c(1,2))
plot(tree_boosting, i.var = "rm", col = "blue")
plot(tree_boosting, i.var = "AP_avg", col = "blue")
plot(tree_boosting, i.var = "AS_avg", col = "firebrick")
plot(tree_boosting, i.var = "AP_avg", col = "blue")
plot(tree_boosting, i.var = "AS_avg", col = "firebrick")
pred_train <- predict(tree_boosting, newdata = train)
pred_test <- predict(tree_boosting, newdata = test)
MSE.rt.train <- mean((train$FTAG_avg - pred_train)^2)
MSE.rt.test <- mean((test$FTAG_avg - pred_test)^2)
plot(test$FTAG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(pred_test, type = "p", col = "blue")
error_test<-test$FTAG_avg-pred_test
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.25, col=c('blue'))
plot(test$FTAG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(pred_test, type = "p", col = "blue")
plot(test$FTAG_avg,type = "p",col = "red", xlab = "Sample",
ylab = "medv Value",
main = "Test: Real (red) - Predicted (blue)")
## lines(test$pred, type = "p", col = "blue") #  option for add_prediction
lines(pred_test, type = "p", col = "blue")
#Histogram of residuals
ggplot(data=as.data.frame(error_test), mapping= aes(x=error_test))+
geom_histogram(binwidth=0.25, col=c('blue'))
# Compare predictions vs real values
plot(test$FTAG_avg,pred_test,col='red',
main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
